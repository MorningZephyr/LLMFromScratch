{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Downloading the data\n",
    "!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data and Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading the data\n",
    "with open('input.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
      "65\n"
     ]
    }
   ],
   "source": [
    "# Finding all unique characters in alphabetical order\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "print(\"\".join(chars))\n",
    "print(vocab_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20, 43, 50, 50, 53, 6, 1, 35, 53, 56, 50, 42, 2]\n",
      "Hello, World!\n"
     ]
    }
   ],
   "source": [
    "# Creating a mapping from chars to int, and vice versa\n",
    "stoi = {ch:i for i, ch in enumerate(chars)}\n",
    "itos = {i:ch for i, ch in enumerate(chars)}\n",
    "\n",
    "# encoder and decoder functions\n",
    "encode = lambda s: [stoi[ch] for ch in s ]              # returns a list of encoded chars\n",
    "decode = lambda l: ''.join([itos[i] for i in l])        # returns the string\n",
    "\n",
    "print(encode(\"Hello, World!\"))\n",
    "print(decode(encode(\"Hello, World!\")))                  # decode(encode(s)) will return s\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1115394]) torch.int64\n",
      "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
      "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
      "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
      "        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n",
      "         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n",
      "        58, 47, 64, 43, 52, 10,  0, 37, 53, 59,  1, 39, 56, 43,  1, 39, 50, 50,\n",
      "         1, 56, 43, 57, 53, 50, 60, 43, 42,  1, 56, 39, 58, 46, 43, 56,  1, 58,\n",
      "        53,  1, 42, 47, 43,  1, 58, 46, 39, 52,  1, 58, 53,  1, 44, 39, 51, 47,\n",
      "        57, 46, 12,  0,  0, 13, 50, 50, 10,  0, 30, 43, 57, 53, 50, 60, 43, 42,\n",
      "         8,  1, 56, 43, 57, 53, 50, 60, 43, 42,  8,  0,  0, 18, 47, 56, 57, 58,\n",
      "         1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 18, 47, 56, 57, 58,  6,  1, 63,\n",
      "        53, 59,  1, 49, 52, 53, 61,  1, 15, 39, 47, 59, 57,  1, 25, 39, 56, 41,\n",
      "        47, 59, 57,  1, 47, 57,  1, 41, 46, 47, 43, 44,  1, 43, 52, 43, 51, 63,\n",
      "         1, 58, 53,  1, 58, 46, 43,  1, 54, 43, 53, 54, 50, 43,  8,  0,  0, 13,\n",
      "        50, 50, 10,  0, 35, 43,  1, 49, 52, 53, 61,  5, 58,  6,  1, 61, 43,  1,\n",
      "        49, 52, 53, 61,  5, 58,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47, 58,\n",
      "        47, 64, 43, 52, 10,  0, 24, 43, 58,  1, 59, 57,  1, 49, 47, 50, 50,  1,\n",
      "        46, 47, 51,  6,  1, 39, 52, 42,  1, 61, 43,  5, 50, 50,  1, 46, 39, 60,\n",
      "        43,  1, 41, 53, 56, 52,  1, 39, 58,  1, 53, 59, 56,  1, 53, 61, 52,  1,\n",
      "        54, 56, 47, 41, 43,  8,  0, 21, 57,  5, 58,  1, 39,  1, 60, 43, 56, 42,\n",
      "        47, 41, 58, 12,  0,  0, 13, 50, 50, 10,  0, 26, 53,  1, 51, 53, 56, 43,\n",
      "         1, 58, 39, 50, 49, 47, 52, 45,  1, 53, 52,  5, 58, 11,  1, 50, 43, 58,\n",
      "         1, 47, 58,  1, 40, 43,  1, 42, 53, 52, 43, 10,  1, 39, 61, 39, 63,  6,\n",
      "         1, 39, 61, 39, 63,  2,  0,  0, 31, 43, 41, 53, 52, 42,  1, 15, 47, 58,\n",
      "        47, 64, 43, 52, 10,  0, 27, 52, 43,  1, 61, 53, 56, 42,  6,  1, 45, 53,\n",
      "        53, 42,  1, 41, 47, 58, 47, 64, 43, 52, 57,  8,  0,  0, 18, 47, 56, 57,\n",
      "        58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 35, 43,  1, 39, 56, 43,  1,\n",
      "        39, 41, 41, 53, 59, 52, 58, 43, 42,  1, 54, 53, 53, 56,  1, 41, 47, 58,\n",
      "        47, 64, 43, 52, 57,  6,  1, 58, 46, 43,  1, 54, 39, 58, 56, 47, 41, 47,\n",
      "        39, 52, 57,  1, 45, 53, 53, 42,  8,  0, 35, 46, 39, 58,  1, 39, 59, 58,\n",
      "        46, 53, 56, 47, 58, 63,  1, 57, 59, 56, 44, 43, 47, 58, 57,  1, 53, 52,\n",
      "         1, 61, 53, 59, 50, 42,  1, 56, 43, 50, 47, 43, 60, 43,  1, 59, 57, 10,\n",
      "         1, 47, 44,  1, 58, 46, 43, 63,  0, 61, 53, 59, 50, 42,  1, 63, 47, 43,\n",
      "        50, 42,  1, 59, 57,  1, 40, 59, 58,  1, 58, 46, 43,  1, 57, 59, 54, 43,\n",
      "        56, 44, 50, 59, 47, 58, 63,  6,  1, 61, 46, 47, 50, 43,  1, 47, 58,  1,\n",
      "        61, 43, 56, 43,  0, 61, 46, 53, 50, 43, 57, 53, 51, 43,  6,  1, 61, 43,\n",
      "         1, 51, 47, 45, 46, 58,  1, 45, 59, 43, 57, 57,  1, 58, 46, 43, 63,  1,\n",
      "        56, 43, 50, 47, 43, 60, 43, 42,  1, 59, 57,  1, 46, 59, 51, 39, 52, 43,\n",
      "        50, 63, 11,  0, 40, 59, 58,  1, 58, 46, 43, 63,  1, 58, 46, 47, 52, 49,\n",
      "         1, 61, 43,  1, 39, 56, 43,  1, 58, 53, 53,  1, 42, 43, 39, 56, 10,  1,\n",
      "        58, 46, 43,  1, 50, 43, 39, 52, 52, 43, 57, 57,  1, 58, 46, 39, 58,  0,\n",
      "        39, 44, 44, 50, 47, 41, 58, 57,  1, 59, 57,  6,  1, 58, 46, 43,  1, 53,\n",
      "        40, 48, 43, 41, 58,  1, 53, 44,  1, 53, 59, 56,  1, 51, 47, 57, 43, 56,\n",
      "        63,  6,  1, 47, 57,  1, 39, 57,  1, 39, 52,  0, 47, 52, 60, 43, 52, 58,\n",
      "        53, 56, 63,  1, 58, 53,  1, 54, 39, 56, 58, 47, 41, 59, 50, 39, 56, 47,\n",
      "        57, 43,  1, 58, 46, 43, 47, 56,  1, 39, 40, 59, 52, 42, 39, 52, 41, 43,\n",
      "        11,  1, 53, 59, 56,  0, 57, 59, 44, 44, 43, 56, 39, 52, 41, 43,  1, 47,\n",
      "        57,  1, 39,  1, 45, 39, 47, 52,  1, 58, 53,  1, 58, 46, 43, 51,  1, 24,\n",
      "        43, 58,  1, 59, 57,  1, 56, 43, 60, 43, 52, 45, 43,  1, 58, 46, 47, 57,\n",
      "         1, 61, 47, 58, 46,  0, 53, 59, 56,  1, 54, 47, 49, 43, 57,  6,  1, 43,\n",
      "        56, 43,  1, 61, 43,  1, 40, 43, 41, 53, 51, 43,  1, 56, 39, 49, 43, 57,\n",
      "        10,  1, 44, 53, 56,  1, 58, 46, 43,  1, 45, 53, 42, 57,  1, 49, 52, 53,\n",
      "        61,  1, 21,  0, 57, 54, 43, 39, 49,  1, 58, 46, 47, 57,  1, 47, 52,  1,\n",
      "        46, 59, 52, 45, 43, 56,  1, 44, 53, 56,  1, 40, 56, 43, 39, 42,  6,  1,\n",
      "        52, 53, 58,  1, 47, 52,  1, 58, 46, 47, 56, 57, 58,  1, 44, 53, 56,  1,\n",
      "        56, 43, 60, 43, 52, 45, 43,  8,  0,  0])\n"
     ]
    }
   ],
   "source": [
    "# Using torch.Tensor to store entire text dataset (encoded)\n",
    "import torch \n",
    "\n",
    "# dtype is long because it's int64, rather than the default float32\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "print(data.shape, data.dtype)\n",
    "print(data[:1000])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note:\n",
    "- storing in torch.Tensor allows a wide range of tools that comes with pytorch\n",
    "- we make the dtye=torch.long because it's int64 rather than the default float32. Since the encoder only has integer, there's no need to waste extra space for decimal precision\n",
    "\n",
    "Why do we encode the input?\n",
    "- Since computers can really only understand numbers, we have to numberfy the inputs into numbers so that they can be interpreted by the computer\n",
    "- We set words/characters to numbers so that the number will the representative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting up the data into training set and validation sets\n",
    "n = int(0.9*len(data))\n",
    "\n",
    "train_data = data[:n]           # Used for training\n",
    "val_data = data[n:]             # Used to test how good the model is\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note:\n",
    "- When we create a model, we want it to also solve problems outside of its training dataset. If a model is only good at doing what it's trained on, then it can't be generalized, which is useless\n",
    "- That is why we leave 10% of the data for validation purposes, to see how the model performs when it encounters something outside of its training set, or in other words, to see to what extent is the model overfitting\n",
    "- Overfitting is bad, because even if the model can predict the training set to a high degree, it'll not translate well to data that it has never seen before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Given tensor([18]), my target is 47\n",
      "Given tensor([18, 47]), my target is 56\n",
      "Given tensor([18, 47, 56]), my target is 57\n",
      "Given tensor([18, 47, 56, 57]), my target is 58\n",
      "Given tensor([18, 47, 56, 57, 58]), my target is 1\n",
      "Given tensor([18, 47, 56, 57, 58,  1]), my target is 15\n",
      "Given tensor([18, 47, 56, 57, 58,  1, 15]), my target is 47\n",
      "Given tensor([18, 47, 56, 57, 58,  1, 15, 47]), my target is 58\n"
     ]
    }
   ],
   "source": [
    "# Selecting blocks \n",
    "block_size = 8\n",
    "x = train_data[:block_size]\n",
    "y = train_data[1:block_size+ 1]\n",
    "for t in range(block_size):\n",
    "    print(f\"Given {x[:t+1]}, my target is {y[t]}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note:\n",
    "- We pick a small block size, or context length to train data because loading all the dataset at once is impractical, such as limited memory\n",
    "- We select random chunks of the dataset and then train them. Those chunks have a max size\n",
    "\n",
    "Numbers of training data in a block:\n",
    "- When selecting a chunk/block from the dataset, there's actually mulitple training data packed into. See the above code\n",
    "- In a way, this is a bayesian framework of conditional probability!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs: torch.Size([4, 8])\n",
      "tensor([[24, 43, 58,  5, 57,  1, 46, 43],\n",
      "        [44, 53, 56,  1, 58, 46, 39, 58],\n",
      "        [52, 58,  1, 58, 46, 39, 58,  1],\n",
      "        [25, 17, 27, 10,  0, 21,  1, 54]])\n",
      "\n",
      "targets: torch.Size([4, 8])\n",
      "tensor([[43, 58,  5, 57,  1, 46, 43, 39],\n",
      "        [53, 56,  1, 58, 46, 39, 58,  1],\n",
      "        [58,  1, 58, 46, 39, 58,  1, 46],\n",
      "        [17, 27, 10,  0, 21,  1, 54, 39]])\n",
      "----------\n",
      "Given tensor([24]), my target is 43\n",
      "Given tensor([24, 43]), my target is 58\n",
      "Given tensor([24, 43, 58]), my target is 5\n",
      "Given tensor([24, 43, 58,  5]), my target is 57\n",
      "Given tensor([24, 43, 58,  5, 57]), my target is 1\n",
      "Given tensor([24, 43, 58,  5, 57,  1]), my target is 46\n",
      "Given tensor([24, 43, 58,  5, 57,  1, 46]), my target is 43\n",
      "Given tensor([24, 43, 58,  5, 57,  1, 46, 43]), my target is 39\n",
      "Given tensor([44]), my target is 53\n",
      "Given tensor([44, 53]), my target is 56\n",
      "Given tensor([44, 53, 56]), my target is 1\n",
      "Given tensor([44, 53, 56,  1]), my target is 58\n",
      "Given tensor([44, 53, 56,  1, 58]), my target is 46\n",
      "Given tensor([44, 53, 56,  1, 58, 46]), my target is 39\n",
      "Given tensor([44, 53, 56,  1, 58, 46, 39]), my target is 58\n",
      "Given tensor([44, 53, 56,  1, 58, 46, 39, 58]), my target is 1\n",
      "Given tensor([52]), my target is 58\n",
      "Given tensor([52, 58]), my target is 1\n",
      "Given tensor([52, 58,  1]), my target is 58\n",
      "Given tensor([52, 58,  1, 58]), my target is 46\n",
      "Given tensor([52, 58,  1, 58, 46]), my target is 39\n",
      "Given tensor([52, 58,  1, 58, 46, 39]), my target is 58\n",
      "Given tensor([52, 58,  1, 58, 46, 39, 58]), my target is 1\n",
      "Given tensor([52, 58,  1, 58, 46, 39, 58,  1]), my target is 46\n",
      "Given tensor([25]), my target is 17\n",
      "Given tensor([25, 17]), my target is 27\n",
      "Given tensor([25, 17, 27]), my target is 10\n",
      "Given tensor([25, 17, 27, 10]), my target is 0\n",
      "Given tensor([25, 17, 27, 10,  0]), my target is 21\n",
      "Given tensor([25, 17, 27, 10,  0, 21]), my target is 1\n",
      "Given tensor([25, 17, 27, 10,  0, 21,  1]), my target is 54\n",
      "Given tensor([25, 17, 27, 10,  0, 21,  1, 54]), my target is 39\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1337)\n",
    "batch_size = 4          # How many independent chunks should we process in parallel\n",
    "block_size = 8          # Maximum context length of per chunk\n",
    "\n",
    "def get_batch(split: str) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"The function samples a batch of data and its respective targets(y) and returns both as 2-D torch.Tensor\"\"\"\n",
    "\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "\n",
    "    # We're just implementing the ideas from above cell\n",
    "    x = torch.stack([data[i:i + block_size] for i in ix])                   # x values to predict the next token\n",
    "    y = torch.stack([data[i+1: i + block_size+1] for i in ix])              # y values, the target\n",
    "\n",
    "    return x,y\n",
    " \n",
    "xb, yb = get_batch('train')\n",
    "\n",
    "print('inputs:',xb.shape)\n",
    "print(xb)\n",
    "print()\n",
    "print('targets:', yb.shape)\n",
    "print(yb)\n",
    "\n",
    "print(\"----------\")\n",
    "\n",
    "# Our training set, comprised of input and target\n",
    "for i in range(batch_size):             # iterating each of the batch           , batch dimension\n",
    "    for j in range(block_size):         # iterating each of the training data   , time dimension\n",
    "        print(f\"Given {xb[i][:j+1]}, my target is {yb[i][j]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Elaborations:\n",
    "\n",
    "`ix = torch.randint(len(data) - block_size, (batch_size,))`\n",
    "- Here we are generating `batch_size` amount of random integers to serve as the starting index for getting chunks, where these integers are in [0, `len(data)-block_size`)\n",
    "- The range is so that we don't go oob when we try to get `block_size` amount of data from a starting index\n",
    "- The random indices will be stored in a 1D tensor, as denoted by `(batch_size,)`\n",
    "- The tuple notation signifys that it's a 1D tensor, and if we have something else like `(2,3)`, then that'll be a 2D tensor with dimensions 2X3, meaning 2 rows, each having length 3\n",
    "- If we were to write `(batch_size)` without the ',', then it'll be a scalar tensor, which doesn't make sense if I want to sample random integers and storing those collections in a container\n",
    "\n",
    "---\n",
    "`torch.stack`\n",
    "- A torch stack is when you stack a bunch tensors of SAME dimension n, and it'll become a new tensor with dimension n+1\n",
    "- For example, if I have `tensor(1)`, `tensor(2)`, `tensor(3)`, all 0D, and I stack them via torch.stack, then the resulting tensor will be a 1D vector tensor that looks like `tensor([1,2,3])`\n",
    "- If I have `tensor([1,2,3])` and `tensor([4,5,6])`, then stack will result in a 2X3 matrix\n",
    "- When it comes to training data, we stack those 1D vectors so that each vectors will be trained simultaneously via the parallel processing nature of GPU\n",
    "\n",
    "ex. Training in parallel\n",
    "\n",
    "|   <------ <1,2,3>         my independent vector1\n",
    "\n",
    "|   <------ <4,5,6>         my independent vector2\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "`x = torch.stack([data[i:i + block_size] for i in ix])` \n",
    "- For each of the random indices `i`, we sample a chunk [`i` to `index + block_size`)\n",
    "- ix is a 1D tensor, so the variable `i` will be each of the 0D/scalar tensor contained within it. A scalar tensor, `tensor(k)`, is not the same as the number `k`, so we can't use scalar tensor to index a list. We would have to use `tensor.item()` to retrieve the number and then use it to index. However, since the 'list' we're trying to index is another tensor, pytorch takes care of the conversions!\n",
    "- After we have the multiple sample chunks, we stack them into a `batch_size`X`block_size` tensor, so a 2D matrix for parallel processing\n",
    "\n",
    "`y = torch.stack([data[i+1: i + block_size+1] for i in ix])`\n",
    "- Same idea from above, except that the `y` will be the target of `x` when it comes to training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Without target: torch.Size([4, 8, 65])\n",
      "With targets: torch.Size([32, 65])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn                     # Template for neural network\n",
    "from torch.nn import functional as F      # For the loss function; measures the performance of the model\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "class BigramLanguageModel(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        # Creates embedding table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
    "        \n",
    "    def forward(self, idx, targets=None):\n",
    "        \n",
    "        # IN THIS IMPLEMENTATION, THE TABLE IS ALSO WHERE WE GET LOGITS\n",
    "        logits = self.token_embedding_table(idx)\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "\n",
    "\n",
    "        # SO THE BOTTOM IS FOR EVALUATING THE LOSS FUNCTION, THAT'S WHEN WE COLLAPSE THE B x T\n",
    "        else:\n",
    "            # This conversion is because F.cross_entropy only takes 2D tensors\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)         # Converting targets to 1D\n",
    "\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "        return logits, loss\n",
    "\n",
    "    # The generate function is only using the one parameter version of the forward pass!!!!!!!\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # calls the foward function, DOESN'T REQUIRE TARGET\n",
    "            logits, loss = self(idx)\n",
    "            print(logits.shape)\n",
    "            #Taking the token from each batch, which is also the logits in our bigram model\n",
    "            logits = logits[:, -1, :]\n",
    "            # using softmax to normalize the probability\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "\n",
    "            # Getting the next token (in the integer form)\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)\n",
    "\n",
    "            # Concatenate the token (in its integer form) to the time step via first dim (See notes)\n",
    "            idx = torch.cat((idx, idx_next), dim=1)\n",
    "        \n",
    "\n",
    "        # This doesn't return the decoded version, but rather encoded one\n",
    "        return idx\n",
    "\n",
    "    \n",
    "m = BigramLanguageModel(vocab_size)\n",
    "\n",
    "                # Calls the forward pass\n",
    "logits, target = m(xb)\n",
    "print(\"Without target:\", logits.shape)\n",
    "logits, target = m(xb, yb)\n",
    "print(\"With targets:\", logits.shape)\n",
    "#Vprint(decode(m.generate(idx = torch.zeros((1, 1), dtype=torch.long), max_new_tokens=100)[0].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neural Network Structure\n",
    "---\n",
    "\n",
    "Foward Pass\n",
    "-----\n",
    "The forward pass is the process of passing inputs through the neural network to produce an output. Depending on the path/flow the input travels, the output will change\n",
    "- The network is comprised of multiple layers. Inputs travel through the layers until the final layer, which is size one (1 output)\n",
    "- Given a layer `i`, its inputs will be the output of layer `i-1`. Layer `i`'s output will be the input of layer `i+1`\n",
    "\n",
    "\n",
    "\n",
    "Vectors Embedding\n",
    "---\n",
    "Inputs are embedded into vectors to carry semantic meaning and attributes of a word. The reason why those inputs are not represented by a number, such as in hashing, is because a single number is too simple and can't express the multi-layered depth of a word. \n",
    "\n",
    "For example: `{1 : 'cat', 2 : 'dog', 3 : 'apple'}`\n",
    "\n",
    "- The model will intrepet `cat` and `dog` as similar due to their numeric proximity, which is true intuitively, since both are animals\n",
    "- However, since `dog` and `apple` have the close proximity, the ai would interpret them as being similar, which is not the case!\n",
    "- Embedding a word via vector gives more depth to it, allowing proper semantic relationship and meaning to puncture through\n",
    "\n",
    "\n",
    "Breakdown of codes\n",
    "---\n",
    "`logits = logits.view(B*T, C)`\n",
    "\n",
    "- This has to do with the function `F.cross_entropy(logits, targets)`, as it expects the tensor dimsion of the parameters to be 2D. Logits are in the shape BxTxC, which is 3D. The idea then is to append all batches together into a single large list\n",
    "\n",
    "`self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)`\n",
    "\n",
    "- This creates a lookup table for each token, where each token is converted to higher dimension vector to better capture the token's meaning. Here the dimension size of the vector happens to be vocab_size as well, but not always\n",
    "\n",
    "`logits = self.token_embedding_table(idx)`\n",
    "\n",
    "- `idx` is the batches of text we sampled, and for each token that `idx` contains, it'll get converted to their corresponding vector representation. If `idx` is dimension B x T, then the output will become B x T x C, where C is the embedding dimension size of the vectors. In addition, IN THIS VERSION OF BIGRAM MODEL, THE LOGITS IS OBTAINED DIRECTLY FROM THE LOOK UP TABLE.\n",
    "\n",
    "`loss = F.cross_entropy(logits, targets)`\n",
    "\n",
    "- `F.cross_entropy` measures how well the model predicts the targets. Within, `cross_entropy` will softmax the `logits`, resulting in a probability space. If the ground truth (target) is a token of index `i`, then `cross_entropy` will select the corresponding index probability and calculate the negative log of that probability\n",
    "\n",
    "- L = -log(probability corresponding to index i)\n",
    "\n",
    "- ***The lower the loss, the more likely the model will select the right token, the more accurate the model it is***\n",
    "\n",
    "`logits = logits[:, -1, :]`\n",
    "- This syntax in pytorch tells python to take all the batches, and from each batch, only look at the last element (last time step), and take all of its components (class or aka vocab size)\n",
    "- We do this in Bigram model, as we use the last token to predict the next, thus it hasn't explored any of the contexts beyond the last token, which will not make the model as effective\n",
    "\n",
    "`probs = F.softmax(logits, dim=-1)`\n",
    "- This is to apply probability normalization across the last dimension, as specified in `dim=-1`\n",
    "- This makes sense as the last dimension is the logits of eacch of the tokens. \n",
    "\n",
    "`idx_next = torch.multinomial(probs, num_samples=1)`\n",
    "- The `torch.multinomial` function takes in a 2D tensor (Batch x Class) and will sample `num_samples` amount for EACH Batch. Thus, the result will be (B x `num_samples`)\n",
    "- Higher probability doesn't mean it'll be chosen always, it just have a higher chance\n",
    "- It is important to note that the function samples and select the indices, and the indices will get mapped to their respective token later\n",
    "\n",
    "`idx = torch.cat((idx, idx_next), dim=1)`\n",
    "- Since `idx` is B x T and `idx_next` is B x 1, what we do is concatenate them via the first dimension. It DOESN'T mean via B (batch) dimension, but the T (time step)\n",
    "- The dimension count starts at `0`, so B is the `0th` index, and T is the `1st` dimension"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
